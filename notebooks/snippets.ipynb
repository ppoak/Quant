{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this snippets notebook, I provided some useful functions while I'm using python, including the some crawler, quantative trading, and database constructing area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Memory Usage\n",
    "\n",
    "To reduce the memory of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\"iterate through all the columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum()\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    end_mem = df.memory_usage().sum()\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Wrapper\n",
    "\n",
    "To save some middle result to cache for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from diskcache import Cache\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "def cache_wrapper(directory: str = './cache/', expire: int = 3600):\n",
    "    cache = Cache(directory=directory)\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            key = func.__name__ + ':' + hashlib.md5((func.__name__ + str(args) + str(kwargs)).encode('utf-8')).hexdigest()\n",
    "            result = cache.get(key=key)\n",
    "            if result is not None:\n",
    "                return result\n",
    "            result = func(*args, **kwargs)\n",
    "            cache.set(key=key, value=result, expire=expire)\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Proxy\n",
    "\n",
    "Getting some proxies for your crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_proxy(page_size: int = 20):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"\n",
    "    }\n",
    "    url_list = [f'https://free.kuaidaili.com/free/inha/{i}/' for i in range(1, page_size + 1)]\n",
    "    proxies = []\n",
    "    for url in url_list:\n",
    "        data = pd.read_html(url)[0][['IP', 'PORT', '类型']].drop_duplicates()\n",
    "        print(f'[+] {url} Get Success!')\n",
    "        data['类型'] = data['类型'].str.lower()\n",
    "        proxy = (data['类型'] + '://' + data['IP'] + ':' + data['PORT'].astype('str')).to_list()\n",
    "        proxies += list(map(lambda x: {x.split('://')[0]: x}, proxy))\n",
    "        time.sleep(0.8)\n",
    "    available_proxies = []\n",
    "    \n",
    "    for proxy in proxies:\n",
    "        try:\n",
    "            res = requests.get('https://www.baidu.com', \n",
    "                headers=headers, proxies=proxy, timeout=1)\n",
    "            res.raise_for_status()\n",
    "            available_proxies.append(proxy)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    \n",
    "    print(f'[=] Get {len(proxies)} proxies, while {len(available_proxies)} are available. '\n",
    "        f'Current available rate is {len(available_proxies) / len(proxies) * 100:.2f}%')\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese Holidays\n",
    "\n",
    "To get chinese holiday for constructing a pd.DateOffset, which will help you in chinese market trading date frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def chinese_holidays():\n",
    "    root = 'https://api.apihubs.cn/holiday/get'\n",
    "    complete = False\n",
    "    page = 1\n",
    "    holidays = []\n",
    "    while not complete:\n",
    "        params = f'?field=date&holiday_recess=1&cn=1&page={page}&size=366'\n",
    "        url = root + params\n",
    "        data = requests.get(url, verbose=False).get().json['data']\n",
    "        if data['page'] * data['size'] >= data['total']:\n",
    "            complete = True\n",
    "        days = pd.DataFrame(data['list']).date.astype('str')\\\n",
    "            .astype('datetime64[ns]').to_list()\n",
    "        holidays += days\n",
    "        page += 1\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proxy Request\n",
    "\n",
    "A request method tries to get data by proxy, but differently, it will not stop requesting until the website is successfully got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import requests\n",
    "\n",
    "def proxy_request(\n",
    "    url: str, \n",
    "    proxies: 'dict | list', \n",
    "    retry: int = None, \n",
    "    timeout: int = 1,\n",
    "    delay: int = 0,\n",
    "    verbose: bool = True,\n",
    "    **kwargs\n",
    "):\n",
    "    if isinstance(proxies, dict):\n",
    "        proxies = [proxies]\n",
    "    retry = retry or len(proxies)\n",
    "    random.shuffle(proxies) \n",
    "    for try_times, proxy in enumerate(proxies):\n",
    "        if try_times + 1 <= retry:\n",
    "            try:\n",
    "                response = requests.get(url, proxies=proxy, timeout=timeout, **kwargs)\n",
    "                response.raise_for_status()\n",
    "                if verbose:\n",
    "                    print(f'[+] {url}, try {try_times + 1}/{retry}')\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f'[-] [{e}] {url}, try {try_times + 1}/{retry}')\n",
    "                time.sleep(delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese Stock Code Wrapper\n",
    "\n",
    "This function can help you wrap the bare stock code with market identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def wrap_code(\n",
    "    code: str, \n",
    "    formatstr: str = '{code}.{market}', \n",
    "    style: str = 'wind',\n",
    "):\n",
    "    if not code.isdigit():\n",
    "        raise ValueError('It seems your code has already been wrapped')\n",
    "    \n",
    "    sh_code_pat = '6\\d{5}|9\\d{5}'\n",
    "    sz_code_pat = '0\\d{5}|2\\d{5}|3\\d{5}'\n",
    "    bj_code_pat = '8\\d{5}|4\\d{5}'\n",
    "\n",
    "    if style == 'wind':\n",
    "        sh_market = 'SH'\n",
    "        sz_market = 'SZ'\n",
    "        bj_market = 'BJ'\n",
    "    elif style == 'rq' or style == 'jq':\n",
    "        sh_market = 'XSHG'\n",
    "        sz_market = 'XSHE'\n",
    "        bj_market = ''\n",
    "    else:\n",
    "        raise ValueError('Style must be one of `wind`, `rq` or `jq`')\n",
    "        \n",
    "    if re.match(sh_code_pat, code):\n",
    "        return formatstr.format(code=code, market='sh')\n",
    "    if re.match(sz_code_pat, code):\n",
    "        return formatstr.format(code=code, market='sz')\n",
    "    if re.match(bj_code_pat, code):\n",
    "        return formatstr.format(code=code, market='bj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StockUS Crawler\n",
    "\n",
    "A crawler designed to get data from [stockus](https://stock.us), providing get index price and get stock price, get report list and search for report fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "class StockUS:\n",
    "    \n",
    "    __root = \"https://api.stock.us/api/v1/\"\n",
    "    headers = {\n",
    "        \"Host\": \"api.stock.us\",\n",
    "        \"Origin\": \"https://stock.us\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.6 Safari/605.1.15\",\n",
    "        \"Accept-Language\": \"zh-CN\",\n",
    "    }\n",
    "    category = {\n",
    "        1: \"宏观经济\",\n",
    "        2: \"投资策略\",\n",
    "        3: \"行业研究\",\n",
    "        4: \"晨会早报\",\n",
    "        8: \"金工量化\",\n",
    "        9: \"债券研究\",\n",
    "        10: \"期货研究\",\n",
    "    }\n",
    "    todaystr = datetime.datetime.today().strftime(r'%Y%m%d')\n",
    "            \n",
    "    @classmethod\n",
    "    def index_price(\n",
    "        cls, \n",
    "        index: str, \n",
    "        start: str = None, \n",
    "        end: str = None,\n",
    "    ):\n",
    "        start = start or '19900101'\n",
    "        end = end or cls.todaystr\n",
    "        url = cls.__root + f\"index-price?security_code={index}&start={start}&stop={end}\"\n",
    "        res = requests.get(url, headers=cls.headers).json()\n",
    "        price = pd.DataFrame(res['price'])\n",
    "        price['date'] = price['date'].astype('datetime64[ns]')\n",
    "        price = price.set_index('date')\n",
    "        return price\n",
    "    \n",
    "    @classmethod\n",
    "    def cn_price(\n",
    "        cls, \n",
    "        code: str, \n",
    "        start: str = None,\n",
    "        end: str = None,\n",
    "    ):\n",
    "        start = start or '19900101'\n",
    "        end = end or cls.todaystr\n",
    "        url = cls.__root + f\"cn-price?security_code={code}&start={start}&stop={end}\"\n",
    "        res = requests.get(url, headers=cls.headers).json()\n",
    "        price = pd.DataFrame(res['price'])\n",
    "        price['date'] = price['date'].astype('datetime64[ns]')\n",
    "        price = price.set_index('date')\n",
    "        return price\n",
    "    \n",
    "    @classmethod\n",
    "    def report_list(\n",
    "        cls, \n",
    "        category: str = 8,\n",
    "        sub_category: str = 0,\n",
    "        keyword: str = '', \n",
    "        period: str = 'all', \n",
    "        org_name: str = '', \n",
    "        author: str = '',\n",
    "        xcf_years: str = '', \n",
    "        search_fields: str = 'title',\n",
    "        page: int = 1, \n",
    "        page_size: int = 100\n",
    "    ):\n",
    "        '''Get report data in quant block\n",
    "        ---------------------------------------\n",
    "        category: str, category to the field, use StockUS.category to see possible choices\n",
    "        keyword: str, key word to search, default empty string to list recent 100 entries\n",
    "        period: str, report during this time period\n",
    "        q: str, search keyword\n",
    "        org_name: str, search by org_name\n",
    "        author: str, search by author\n",
    "        xcf_years: str, search by xcf_years\n",
    "        search_fields: str, search in fields, support \"title\", \"content\", \"content_fp\"\n",
    "        page: int, page number\n",
    "        page_size: int, page size\n",
    "        '''\n",
    "        url = cls.__root + 'research/report-list'\n",
    "        params = (f'?category={category}&dates={period}&q={keyword}&org_name={org_name}'\n",
    "                  f'&author={author}&xcf_years={xcf_years}&search_fields={search_fields}'\n",
    "                  f'&page={page}&page_size={page_size}')\n",
    "        if category != 8:\n",
    "            params += f'&sub_category={sub_category}'\n",
    "        headers = {\n",
    "            \"Referer\": \"https://stock.us/cn/report/quant\",\n",
    "        }\n",
    "        headers.update(cls.headers)\n",
    "        url += params\n",
    "        res = requests.get(url, headers=headers).json()\n",
    "        data = pd.DataFrame(res['data'])\n",
    "        data[['pub_date', 'pub_week']] = data[['pub_date', 'pub_week']].astype('datetime64[ns]')\n",
    "        data.authors = data.authors.map(\n",
    "            lambda x: ' '.join(list(map(lambda y: y['name'] + ('*' if y['prize'] else ''), x))))\n",
    "        data = data.set_index('id')\n",
    "        return data\n",
    "    \n",
    "    @classmethod\n",
    "    def report_search(\n",
    "        cls, \n",
    "        keyword: str = '', \n",
    "        period: str = '3m', \n",
    "        org_name: str = '', \n",
    "        author_name: str = '',\n",
    "        xcf_years: str = '', \n",
    "        search_fields: str = 'title',\n",
    "        page: int = 1, \n",
    "        page_size: int = 100\n",
    "    ):\n",
    "        '''Search report in stockus database\n",
    "        ---------------------------------------\n",
    "        keyword: str, key word to search, default empty string to list recent 100 entries\n",
    "        period: str, report during this time period\n",
    "        org_name: str, search by org_name\n",
    "        author: str, search by author\n",
    "        xcf_years: str, search by xcf_years\n",
    "        search_fields: str, search in fields, support \"title\", \"content\", \"content_fp\"\n",
    "        page: int, page number\n",
    "        page_size: int, page size\n",
    "        '''\n",
    "        url = cls.__root + 'research/report-search'\n",
    "        params = (f'?dates={period}&q={keyword}&org_name={org_name}&author_name={author_name}'\n",
    "                  f'&xcf_years={xcf_years}&search_fields={search_fields}&page={page}'\n",
    "                  f'&page_size={page_size}')\n",
    "        url += params\n",
    "        res = requests.get(url, headers=cls.headers).json()\n",
    "        data = pd.DataFrame(res['data'])\n",
    "        data['pub_date'] = data['pub_date'].astype('datetime64[ns]')\n",
    "        data.authors = data.authors.map(\n",
    "            lambda x: ' '.join(list(map(lambda y: y['name'] + ('*' if y['prize'] else ''), x)))\n",
    "            if isinstance(x, list) else '')\n",
    "        data = data.set_index('id')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cnki crawler\n",
    "\n",
    "This is a Cnki crawler, temporary support for simple search keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "class Cnki:\n",
    "\n",
    "    __search_url = \"https://kns.cnki.net/KNS8/Brief/GetGridTableHtml\"\n",
    "\n",
    "    @classmethod\n",
    "    def generic_search(cls, keyword: str, page: int = 3):\n",
    "        headers = {\n",
    "            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
    "            'Host': 'kns.cnki.net',\n",
    "            'Origin': 'https://kns.cnki.net',\n",
    "            'Referer': 'https://kns.cnki.net/kns8/defaultresult/index',\n",
    "        }\n",
    "        data = {\n",
    "            \"IsSearch\": 'true',\n",
    "            \"QueryJson\": '{\"Platform\":\"\",\"DBCode\":\"SCDB\",\"KuaKuCode\":\"CJFQ,CDMD,CIPD,CCND,BDZK,CISD,SNAD,CCJD,GXDB_SECTION,CJFN,CCVD,CLKLK\",\"QNode\":{\"QGroup\":[{\"Key\":\"Subject\",\"Title\":\"\",\"Logic\":1,\"Items\":[{\"Title\":\"主题\",\"Name\":\"SU\",\"Value\":\"' + f'{keyword}' + '\",\"Operate\":\"%=\",\"BlurType\":\"\"}],\"ChildItems\":[]}]}}',\n",
    "            \"PageName\": 'DefaultResult',\n",
    "            \"DBCode\": 'SCDB',\n",
    "            \"KuaKuCodes\": 'CJFQ,CDMD,CIPD,CCND,BDZK,CISD,SNAD,CCJD,GXDB_SECTION,CJFN,CCVD,CLKLK',\n",
    "            \"SearchSql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n",
    "            \"CurPage\": '1',\n",
    "            \"RecordsCntPerPage\": '50',\n",
    "            \"CurDisplayMode\": 'listmode',\n",
    "            \"CurrSortField\": r'%e5%8f%91%e8%a1%a8%e6%97%b6%e9%97%b4%2f(%e5%8f%91%e8%a1%a8%e6%97%b6%e9%97%b4%2c%27TIME%27)',\n",
    "            \"CurrSortFieldType\": 'desc',\n",
    "            \"IsSentenceSearch\": 'false',\n",
    "            \"Subject\": '',\n",
    "        }\n",
    "        results = []\n",
    "        # first attempt to get result and total pages\n",
    "        req = requests.post(cls.__search_url, headers=headers, data=data)\n",
    "        text = req.text\n",
    "        total = int(re.findall(r'共找到.{0,}?([\\d,]+).{0,}?条结果', text)[0].replace(',', ''))\n",
    "        if page == -1:\n",
    "            page = ceil(total / 50)\n",
    "        if page == 1:\n",
    "            return pd.concat(results, axis=1)\n",
    "        page = min(page, ceil(total / 50))\n",
    "        print(f'[+] Current page 1 / {page}')\n",
    "        result = pd.read_html(text)[0]\n",
    "        results.append(result)\n",
    "\n",
    "        # now if the crawl didn't end, it will enter the cycle for crawling\n",
    "        for p in range(2, page + 1):\n",
    "            data.update(CurPage=f'{p}', IsSearch='false')\n",
    "            req = requests.post(cls.__search_url, headers=headers, data=data)\n",
    "            text = req.text\n",
    "            print(f'[+] Current page {p} / {page}')\n",
    "            result = pd.read_html(text)[0]\n",
    "            results.append(result)\n",
    "        results = pd.concat(results, axis=0)\n",
    "        results = results.drop(results.columns[0], axis=1)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weibo Searcher\n",
    "\n",
    "This is a Weibo Searcher for sumulating the search bar over the weibo website page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import quote\n",
    "\n",
    "\n",
    "class WeiboSearch:\n",
    "    '''A search crawler engine for weibo\n",
    "    ====================================\n",
    "    sample usage:\n",
    "    >>> result = WeiboSearch.search(\"keyword\")\n",
    "    '''\n",
    "\n",
    "    __base = \"https://m.weibo.cn/api/container/getIndex?containerid=100103type%3D1%26q%3D{}&page_type=searchall&page={}\"\n",
    "\n",
    "    @classmethod\n",
    "    def _get_content(cls, url, headers):\n",
    "\n",
    "        def _parse(mblog):\n",
    "            blog = {\n",
    "                \"created_at\": mblog[\"created_at\"],\n",
    "                \"text\": re.sub(r'<(.*?)>', '', mblog['text']),\n",
    "                \"id\": mblog[\"id\"],\n",
    "                \"link\": f\"https://m.weibo.cn/detail/{mblog['id']}\",                    \n",
    "                \"source\": mblog[\"source\"],\n",
    "                \"username\": mblog[\"user\"][\"screen_name\"],\n",
    "                \"reposts_count\": mblog[\"reposts_count\"],\n",
    "                \"comments_count\": mblog[\"comments_count\"],\n",
    "                \"attitudes_count\": mblog[\"attitudes_count\"],\n",
    "                \"isLongText\": mblog[\"isLongText\"],\n",
    "            }\n",
    "            if blog[\"isLongText\"]:\n",
    "                headers = {\n",
    "                    \"Referer\": f\"https://m.weibo.cn/detail/{blog['id']}\",\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Safari/605.1.15\"\n",
    "                }\n",
    "                resp = requests.get(f\"https://m.weibo.cn/statuses/extend?id={blog['id']}\", headers=headers).json()\n",
    "                blog[\"full_text\"] = resp[\"data\"][\"longTextContent\"]\n",
    "            return blog\n",
    "\n",
    "        # First try to get resources\n",
    "        res = requests.get(url, headers=headers).json()\n",
    "        # if it is end\n",
    "        if res.get(\"msg\"):\n",
    "            return False\n",
    "\n",
    "        # if it contains cards\n",
    "        cards = res[\"data\"][\"cards\"]\n",
    "        blogs = []\n",
    "        for card in cards:\n",
    "            # find 'mblog' tag and append to result blogs\n",
    "            mblog = card.get(\"mblog\")\n",
    "            card_group = card.get(\"card_group\")\n",
    "            if card.get(\"mblog\"):\n",
    "                blog = _parse(mblog)\n",
    "                blogs.append(blog)\n",
    "            elif card_group:\n",
    "                for cg in card_group:\n",
    "                    mblog = cg.get(\"mblog\")\n",
    "                    if mblog:\n",
    "                        blog = _parse(mblog)\n",
    "                        blogs.append(blog)\n",
    "        return blogs\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_full(cls, keyword: str):\n",
    "        page = 1\n",
    "        result = []\n",
    "        headers = {\n",
    "            \"Referer\": f\"https://m.weibo.cn/search?containerid=100103type%3D1%26q%3D{quote(keyword, 'utf-8')}\",\n",
    "            \"User-Agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\"\n",
    "            }\n",
    "        print(f\"Start in keyword: {keyword}\")\n",
    "        while True:\n",
    "            print(f\"Getting {keyword}, currently at page: {page} ... \")\n",
    "            url = cls.__base.format(keyword, page)\n",
    "            blogs = cls._get_content(url, headers)\n",
    "            if not blogs:\n",
    "                break\n",
    "            result.extend(blogs)\n",
    "            page += 1\n",
    "            time.sleep(random.randint(5, 8))\n",
    "        print(f\"Finished in keyword: {keyword}!\")\n",
    "        return result\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_assigned(cls, keyword: str, pages: int):\n",
    "        result = []\n",
    "        print(f\"Start in keyword: {keyword}\")\n",
    "        headers = {\n",
    "            \"Referer\": f\"https://m.weibo.cn/search?containerid=100103type%3D1%26q%3D{quote(keyword, 'utf-8')}\",\n",
    "            \"User-Agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\"\n",
    "            }\n",
    "        for page in tqdm(range(1, pages+1)):\n",
    "            print(f\"Getting {keyword}, currently at page: {page} ... \")\n",
    "            url = cls.__base.format(keyword, page)\n",
    "            blogs = cls._get_content(url, headers)\n",
    "            result.extend(blogs)\n",
    "            time.sleep(random.randint(5, 8))\n",
    "        print(f\"Finished in keyword: {keyword}!\")\n",
    "        return result          \n",
    "    \n",
    "    @classmethod\n",
    "    def search(cls, keyword: str, pages: int = -1):\n",
    "        \"\"\"Search for the keyword\n",
    "        --------------------------\n",
    "        \n",
    "        keyword: str, keyword\n",
    "        pages: int, how many pages you want to get, default -1 to all pages\n",
    "        \"\"\"\n",
    "\n",
    "        keyword = keyword.replace('#', '%23')\n",
    "        if pages == -1:\n",
    "            result = cls._get_full(keyword)\n",
    "        else:\n",
    "            result = cls._get_assigned(keyword, pages)\n",
    "        result = pd.DataFrame(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weibo Google Api for Hot Topic Trend\n",
    "\n",
    "This is a hot topic api trend fetcher from google-api, and supporting get the trend data on a given topic or on a given date, returning the specified hot topic trend data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class HotTopic:\n",
    "    \"\"\"A Second Level Crawler for Hot Topic\n",
    "    ========================================\n",
    "    sample usage:\n",
    "    >>> result = HotTopic.search('keyword')\n",
    "    \"\"\"\n",
    "\n",
    "    __list = \"https://google-api.zhaoyizhe.com/google-api/index/mon/list\"\n",
    "    __search = \"https://google-api.zhaoyizhe.com/google-api/index/mon/sec?isValid=ads&keyword={}\"\n",
    "    __trend = \"https://google-api.zhaoyizhe.com/google-api/index/superInfo?keyword={}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def search(cls, keyword: str = None, date: str = None):\n",
    "        if keyword is None and date is None:\n",
    "            url = cls.__list\n",
    "        elif keyword is None and date is not None:\n",
    "            url = cls.__search.format(date)\n",
    "        elif keyword is not None and date is None:\n",
    "            url = cls.__search.format(keyword)\n",
    "        result = requests.get(url).json()\n",
    "        data = result[\"data\"]\n",
    "        data = pd.DataFrame(data)\n",
    "        data = data.drop(\"_id\", axis=1)\n",
    "        return data\n",
    "\n",
    "    @classmethod\n",
    "    def trend(cls, keyword: str):\n",
    "        url = cls.__trend.format(keyword)\n",
    "        result = requests.get(url).json()\n",
    "        data = pd.DataFrame(map(lambda x: x['value'], result), \n",
    "            columns=['datetime', 'hot', 'tag']).set_index('datetime')\n",
    "        return data\n",
    "\n",
    "    @classmethod\n",
    "    def trend_history(cls, keyword: str, freq: str = '3m'):\n",
    "        if freq not in ['1h', '24h', '1m', '3m']:\n",
    "            raise ValueError('Freq parameter must be in [\"1h\", \"24h', \"1m\", \"3m]\")\n",
    "        if freq.endswith('h'):\n",
    "            freq += 'our'\n",
    "        elif freq.endswith('m'):\n",
    "            freq += 'onth'\n",
    "        url = \"https://data.weibo.com/index/ajax/newindex/searchword\"\n",
    "        data = {\n",
    "            \"word\": f\"{keyword}\"\n",
    "        }\n",
    "        headers = {\n",
    "            \"Host\": \"data.weibo.com\",\n",
    "            \"Origin\": \"https://data.weibo.com\",\n",
    "            \"User-Agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/8.0.16(0x18001041) NetType/WIFI Language/zh_CN\",\n",
    "            \"Content-Length\": \"23\",\n",
    "            \"Referer\": \"https://data.weibo.com/index/newindex?visit_type=search\"\n",
    "        }\n",
    "        html = requests.post(url, data=data, headers=headers)\n",
    "        html = BeautifulSoup(html.text, 'html.parser')\n",
    "        res = html.find_all('li')\n",
    "        wids = [(r.attrs[\"wid\"].strip(r'\\\"'), eval('\"' + r.attrs[\"word\"].replace(r'\\\"', '') + '\"')) for r in res]\n",
    "\n",
    "        url = \"https://data.weibo.com/index/ajax/newindex/getchartdata\"\n",
    "        results = []\n",
    "        for wid in wids:\n",
    "            post_params = {\n",
    "                \"wid\": wid[0],\n",
    "                \"dateGroup\": freq\n",
    "            }\n",
    "            res = requests.post(url, data=post_params, headers=headers).json()\n",
    "            data = res[\"data\"]\n",
    "            index = data[0][\"trend\"]['x']\n",
    "            index = list(map(lambda x: x.replace(\"月\", '-').replace(\"日\", ''), index))\n",
    "            volume = data[0][\"trend\"]['s']\n",
    "            result = pd.Series(volume, index=index, name=wid[1])\n",
    "            results.append(result)\n",
    "        results = pd.concat(results, axis=1)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackTrader Fast Strategy\n",
    "\n",
    "A subclass of `bt.Strategy` with some frequently used method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import backtrader as bt\n",
    "\n",
    "\n",
    "class Strategy(bt.Strategy):\n",
    "\n",
    "    def log(self, text: str, datetime: datetime.datetime = None, hint: str = 'INFO'):\n",
    "        datetime = datetime or self.data.datetime.date(0)\n",
    "        print(f'[{hint}] {datetime}: {text}')\n",
    "\n",
    "    def notify_order(self, order: bt.Order):\n",
    "        if order.status in [order.Submitted, order.Accepted, order.Created]:\n",
    "            return\n",
    "\n",
    "        elif order.status in [order.Completed]:\n",
    "            self.log(f'Trade <{order.executed.size}> <{order.info.get(\"name\", \"data\")}> at <{order.executed.price:.2f}>')\n",
    "            self.bar_executed = len(self)\n",
    "\n",
    "        elif order.status in [order.Canceled, order.Margin, order.Rejected, order.Expired]:\n",
    "            self.log('Order canceled, margin, rejected or expired', hint='WARN')\n",
    "\n",
    "        self.order = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas.concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not use `pd.concat` with parameter `axis=1` for large dataset for its low speed, try to use `pd.concat([df1, df2], axis=0, keys=['keyname1', 'keyname2'], names=['idx_level0_name', 'idx_level1_name'])` instead. Concatenating on index or just merge consumes a lot of resources.\n",
    "\n",
    "For more information, please refer to [Official Document](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be461149623e13b126d9ab65bc78e9e52a75e1177f71df321c71dd4fbcccebcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
