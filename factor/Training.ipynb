{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qlib\n",
    "qlib.init(provider_uri = '../data/qlib_day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this unless the first time, use the next cell to load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "feature = pd.read_pickle('../data/intermediate/feature_info/feature_info.pkl')\n",
    "# label = pd.read_parquet('../data/intermediate/forward_return_1d_close_close_normalized.parquet')\n",
    "label = pd.read_parquet('../data/intermediate/forward_return_1d_close_close.parquet')\n",
    "label.index = pd.MultiIndex.from_arrays([\n",
    "    label.index.get_level_values(0), \n",
    "    label.index.get_level_values(1).map(lambda x: x[:6] + ('.SZ' if x.endswith('XSHE') else '.SH'))\n",
    "])\n",
    "data = pd.concat(\n",
    "    [feature, label], \n",
    "    axis=1, \n",
    "    keys=['feature', 'label'], \n",
    "    join='inner',\n",
    "    sort=True\n",
    ")\n",
    "# data.to_parquet('../data/intermediate/feature_info/normalized_dataset.parquet')\n",
    "data.to_parquet('../data/intermediate/feature_info/dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_parquet('../data/intermediate/feature_info/normalized_dataset.parquet')\n",
    "label = pd.read_parquet('../data/intermediate/forward_return/1d_vwap_vwap.parquet')\n",
    "label.columns = ['label']\n",
    "# data = pd.read_parquet('../data/intermediate/feature_info/dataset.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature.loc[:, feature.columns[0]].hist(bins=100)\n",
    "# corr_ts = feature.groupby(level=0).corr()\n",
    "# corr_ts_mean = corr_ts.groupby(level=1).mean()\n",
    "# corr_ts_std = corr_ts.groupby(level=1).std()\n",
    "# corr_t = corr_ts_mean / corr_ts_std\n",
    "# corr_t.replace({np.inf: np.nan}, inplace=True)\n",
    "# corr_t.loc[corr_t.columns].style.background_gradient(cmap='RdYlGn')\n",
    "# from pandas.plotting import scatter_matrix\n",
    "\n",
    "# scatter_matrix(feature.loc['2022-01-04', ['feature_56', 'feature_25']], hist_kwds={\"bins\": 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a DataHandler from Static DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from qlib.data.dataset.handler import DataHandlerLP\n",
    "from qlib.data.dataset.loader import StaticDataLoader\n",
    "from qlib.data.dataset.processor import Processor\n",
    "\n",
    "class StaticHandler(DataHandlerLP):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data,\n",
    "        instruments = None, \n",
    "        start_time = None, \n",
    "        end_time = None, \n",
    "        drop_raw = False, \n",
    "        **kwargs\n",
    "    ):\n",
    "        data_loader = StaticDataLoader(config = data)\n",
    "        super().__init__(\n",
    "            instruments = instruments, \n",
    "            start_time = start_time, \n",
    "            end_time = end_time, \n",
    "            data_loader = data_loader, \n",
    "            drop_raw = drop_raw, \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "class PCADecomp(Processor):\n",
    "    def __init__(self, fit_start_time, fit_end_time):\n",
    "        self.fit_start_time = fit_start_time\n",
    "        self.fit_end_time = fit_end_time\n",
    "    \n",
    "    def fit(self, df):\n",
    "        df = df.loc[self.fit_start_time:self.fit_end_time]\n",
    "        pca = PCA().fit(df['feature'])\n",
    "        self.n_comp = (~(np.cumsum(pca.explained_variance_ratio_) > 0.95)).sum()\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame):\n",
    "        result = PCA(n_components=self.n_comp).fit_transform(df['feature'])\n",
    "        result = pd.DataFrame(\n",
    "            result, index=df.index, \n",
    "            columns=[f'new_feature_{i}' for i in range(self.n_comp)]\n",
    "        )\n",
    "        result = pd.concat(\n",
    "            [result, df['label']], \n",
    "            axis=1, \n",
    "            keys=['feature', 'label']\n",
    "        )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StaticHandler(\n",
    "    data,\n",
    "    # data.loc[:, (slice(None), selected_features.to_list() + ['label'])], \n",
    "    # infer_processors = [PCADecomp(\"2020-01-01\", \"2020-12-31\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Dataset on StaticHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlib.data.dataset import DatasetH\n",
    "\n",
    "dataset = DatasetH(handler=handler, segments={\n",
    "    \"train\": (\"2018-01-01\", \"2018-07-31\"),\n",
    "    # \"valid\": (\"2018-08-01\", \"2018-09-30\"),\n",
    "    # \"test\": (\"2018-10-01\", \"2018-12-31\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data = handler.fetch(data_key=handler.DK_L, col_set='feature')\n",
    "# new_data_corr = new_data.groupby(level=0).corr()\n",
    "# new_data_corr_mean = new_data_corr.groupby(level=1).mean()\n",
    "# new_data_corr_std = new_data_corr.groupby(level=1).std()\n",
    "# new_data_t = (new_data_corr_mean / new_data_corr_std).replace({np.inf: np.nan})\n",
    "# new_data_t.loc[new_data_t.columns].style.background_gradient(cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlib.contrib.model.gbdt import LGBModel\n",
    "\n",
    "evals_result = {}\n",
    "model = LGBModel(\n",
    "    loss = \"mse\",\n",
    "    learning_rate = 0.01,\n",
    "    lambda_l1 = 0.01,\n",
    "    lambda_l2 = 0.001,\n",
    "    max_depth = 5,\n",
    "    num_leaves = 100,\n",
    "    feature_fraction = 1,\n",
    "    bagging_fraction = 0.72,\n",
    "    bagging_freq = 10,\n",
    "    min_data_in_leaf = 100,\n",
    ")\n",
    "model.fit(dataset, num_boost_round=1000, evals_result=evals_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qlib.contrib.model.double_ensemble import DEnsembleModel\n",
    "\n",
    "model = DEnsembleModel(\n",
    "    base_model='gbm',\n",
    "    loss='mse',\n",
    "    num_models=12,\n",
    "    enable_sr=True,\n",
    "    enable_fs=True,\n",
    "    alpha1=1,\n",
    "    alpha2=1,\n",
    "    bins_sr=10,\n",
    "    bins_fs=10,\n",
    "    decay=0.5,\n",
    "    sample_ratios=[i for i in np.arange(0.9, -0.1, -0.1)],\n",
    "    sub_weights=None,\n",
    "    epochs=100,\n",
    ")\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlib.contrib.model.pytorch_nn import DNNModelPytorch\n",
    "\n",
    "model = DNN(\n",
    "    lr = 8e-2,\n",
    "    lr_decay = 0.3,\n",
    "    lr_decay_steps = 100,\n",
    "    optimizer = 'adam',\n",
    "    max_steps = 4000,\n",
    "    batch_size = 500,\n",
    "    GPU = 0,\n",
    "    weight_decay = 4e-4,\n",
    "    pt_model_kwargs = {\n",
    "        'input_dim': 85,\n",
    "        'layers': (512, 512)\n",
    "    },\n",
    ")\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlib.contrib.model.pytorch_lstm import LSTM\n",
    "\n",
    "model = LSTM(\n",
    "    d_feat = 85,\n",
    "    hidden_size = 512,\n",
    "    num_layers = 2,\n",
    "    dropout = 0.95,\n",
    "    n_epochs = 20,\n",
    "    lr = 0.001,\n",
    "    early_stop = 3,\n",
    "    batch_size = 2000,\n",
    "    loss = 'mse',\n",
    ")\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlib.contrib.model.pytorch_alstm import ALSTM\n",
    "\n",
    "model = ALSTM(\n",
    "    d_feat=85,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.95,\n",
    "    n_epochs=20,\n",
    "    lr=0.001,\n",
    "    batch_size=2000,\n",
    "    early_stop=20,\n",
    "    loss='mse',\n",
    "    optimizer='adam',\n",
    ")\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qlib\n",
    "from pathlib import Path\n",
    "from qlib.data.dataset import DatasetH\n",
    "from qlib.contrib.model.gbdt import LGBModel\n",
    "\n",
    "\n",
    "class RollingTrain:\n",
    "    def __init__(\n",
    "        self,\n",
    "        handler, \n",
    "        min_days: int = 20,\n",
    "        max_days: int = 40,\n",
    "        valid_days: int = 10,\n",
    "        pred_days: int = 5,\n",
    "        exp_name: str = 'lgbm'\n",
    "    ) -> None:\n",
    "        \"\"\"A class used for rolling training\n",
    "        ------------------------------------\n",
    "\n",
    "        handler: qlib.data.dataset.handler.DataHandlerLP,\n",
    "            a data handler constructed by loader\n",
    "        min_days: int, minimum days in training dataset\n",
    "        max_days: int, maximum days in training dataset\n",
    "        valid_days: int, the days contained in valid set\n",
    "        pred_days: int, the days contained in predict set\n",
    "        \"\"\"\n",
    "        self.handler = handler\n",
    "        self.min_days = min_days\n",
    "        self.max_days = max_days\n",
    "        self.valid_days = valid_days\n",
    "        self.pred_days = pred_days\n",
    "        self.exp_name = Path(f'../data/intermediate/results/{exp_name}')\n",
    "        self.exp_name.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def rolling(self, model, **kwargs):\n",
    "        \"\"\"This method rolls on the datahandler\n",
    "        ---------------------------------------\n",
    "\n",
    "        model: a pre-initialized model instance, \n",
    "            and the fit, predict method should be implemented\n",
    "        kwargs: other keyword arguments applies to model.fit method\n",
    "        \"\"\"\n",
    "        datetime_index = self.handler.fetch(data_key='infer', col_set='label').index.levels[0]\n",
    "        for i, idx in list(enumerate(datetime_index))[::self.pred_days]:\n",
    "            if i < self.min_days + self.valid_days + self.pred_days - 1:\n",
    "                continue\n",
    "            pred_end_idx = i\n",
    "            pred_start_idx = i - self.pred_days + 1\n",
    "            valid_end_idx = pred_start_idx - 1\n",
    "            valid_start_idx = valid_end_idx - self.valid_days + 1\n",
    "            train_end_idx = valid_start_idx - 1\n",
    "            train_start_idx = max(min(train_end_idx - self.min_days, train_end_idx - self.max_days), 0)\n",
    "            \n",
    "            dataset = DatasetH(handler=self.handler, segments={\n",
    "                \"train\": (datetime_index[train_start_idx], datetime_index[train_end_idx]),\n",
    "                \"valid\": (datetime_index[valid_start_idx], datetime_index[valid_end_idx]),\n",
    "                \"test\": (datetime_index[pred_start_idx], datetime_index[pred_end_idx]),\n",
    "            })\n",
    "\n",
    "            model.fit(dataset, **kwargs)\n",
    "            pred = model.predict(dataset, segment='test')\n",
    "            label_ = label.loc[pred.index]\n",
    "            pred_label = pd.concat([pred, label_], axis=1)\n",
    "            pred_label.columns = ['score', 'label']\n",
    "            filename = \"pred_label_{}_{}\".format(\n",
    "                datetime_index[pred_start_idx].strftime('%Y%m%d'), \n",
    "                datetime_index[pred_end_idx].strftime('%Y%m%d')\n",
    "            )\n",
    "            pred_label.to_pickle(self.exp_name.joinpath(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Training LGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RollingTrain(\n",
    "    handler, \n",
    "    min_days=100, \n",
    "    max_days=120, \n",
    "    valid_days=20, \n",
    "    pred_days=60,\n",
    "    exp_name='lgbm'\n",
    ").rolling(\n",
    "    LGBModel(\n",
    "        loss = \"mse\",\n",
    "        learning_rate = 0.01,\n",
    "        lambda_l1 = 0.01,\n",
    "        lambda_l2 = 0.01,\n",
    "        max_depth = 20,\n",
    "        num_leaves = 1024,\n",
    "        feature_fraction = 1,\n",
    "        bagging_fraction = 0.72,\n",
    "        bagging_freq = 10,\n",
    "        min_data_in_leaf = 100,\n",
    "    ), num_boost_round = 10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Training Double Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlib.contrib.model.double_ensemble import DEnsembleModel\n",
    "\n",
    "RollingTrain(\n",
    "    handler, \n",
    "    min_days=100, \n",
    "    max_days=120, \n",
    "    valid_days=20, \n",
    "    pred_days=10\n",
    ").rolling(\n",
    "    DEnsembleModel(\n",
    "        base_model='gbm',\n",
    "        loss='mse',\n",
    "        num_models=12,\n",
    "        enable_sr=True,\n",
    "        enable_fs=True,\n",
    "        alpha1=1,\n",
    "        alpha2=1,\n",
    "        bins_sr=10,\n",
    "        bins_fs=10,\n",
    "        decay=0.5,\n",
    "        sample_ratios=[i for i in np.arange(0.9, -0.1, -0.1)],\n",
    "        sub_weights=None,\n",
    "        epochs=1000,\n",
    "        # model params\n",
    "        early_stopping_round=50\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Training DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlib.contrib.model.pytorch_nn import DNNModelPytorch\n",
    "\n",
    "RollingTrain(\n",
    "    handler, \n",
    "    min_days=100, \n",
    "    max_days=120, \n",
    "    valid_days=20, \n",
    "    pred_days=60,\n",
    "    exp_name='dnn',\n",
    ").rolling(\n",
    "    DNNModelPytorch(\n",
    "        lr = 8e-2,\n",
    "        lr_decay = 0.3,\n",
    "        lr_decay_steps = 100,\n",
    "        optimizer = 'adam',\n",
    "        max_steps = 4000,\n",
    "        batch_size = 500,\n",
    "        GPU = 0,\n",
    "        weight_decay = 4e-4,\n",
    "        pt_model_kwargs = {\n",
    "            'input_dim': 85,\n",
    "            'layers': (512, 512)\n",
    "        },\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Training LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlib.contrib.model.pytorch_lstm import LSTM\n",
    "\n",
    "RollingTrain(\n",
    "    handler, \n",
    "    min_days=100, \n",
    "    max_days=120, \n",
    "    valid_days=20, \n",
    "    pred_days=60,\n",
    "    exp_name='lstm'\n",
    ").rolling(\n",
    "    model = LSTM(\n",
    "        d_feat = 85,\n",
    "        hidden_size = 512,\n",
    "        num_layers = 2,\n",
    "        dropout = 0.95,\n",
    "        n_epochs = 20,\n",
    "        lr = 0.001,\n",
    "        early_stop = 3,\n",
    "        batch_size = 2000,\n",
    "        loss = 'mse',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Training ALSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlib.contrib.model.pytorch_alstm import ALSTM\n",
    "\n",
    "RollingTrain(\n",
    "    handler, \n",
    "    min_days=100, \n",
    "    max_days=120, \n",
    "    valid_days=20, \n",
    "    pred_days=60,\n",
    "    exp_name='alstm'\n",
    ").rolling(\n",
    "    model = ALSTM(\n",
    "    d_feat=85,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.95,\n",
    "    n_epochs=20,\n",
    "    lr=0.001,\n",
    "    batch_size=2000,\n",
    "    early_stop=20,\n",
    "    loss='mse',\n",
    "    optimizer='adam',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from qlib.utils import init_instance_by_config\n",
    "\n",
    "def objective_lgbm(trial):\n",
    "    task = {\n",
    "        \"model\": {\n",
    "            \"class\": \"LGBModel\",\n",
    "            \"module_path\": \"qlib.contrib.model.gbdt\",\n",
    "            \"kwargs\": {\n",
    "                \"loss\": \"mse\",\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 1, 36, step=5),\n",
    "                \"num_leaves\": trial.suggest_int(\"num_leaves\", 128, 2048, log=True),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "                \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-3, 1e-2, log=True),\n",
    "                \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-3, 1e-2, log=True),\n",
    "                \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.7, 0.8),\n",
    "                \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.7, 0.8),\n",
    "                \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
    "                \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 90, step=10),\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    evals_result = dict()\n",
    "    model = init_instance_by_config(task[\"model\"])\n",
    "    model.fit(dataset, evals_result=evals_result)\n",
    "    pred = model.predict(dataset, segment='test')\n",
    "    label = dataset.prepare(segments='test', col_set='label', data_key='infer').squeeze()\n",
    "    pred_label = pd.concat([pred, label], axis=1)\n",
    "    ic = pred_label.groupby(level=0).corr()\n",
    "    ic = ic.loc[(slice(None), ic.columns[0]), ic.columns[-1]].mean()\n",
    "    return ic\n",
    "\n",
    "def objective_mlp(trial):\n",
    "    nlayers = trial.suggest_int(\"nlayers\", 1, 5)\n",
    "    layers = [trial.suggest_int(f\"layer_{i}\", 256, 2048, log=True) for i in range(nlayers)]\n",
    "    task = {\n",
    "        \"model\": {\n",
    "            \"class\": \"qlib.contrib.model.pytorch_nn.DNNModelPytorch\",\n",
    "            \"kwargs\": {\n",
    "                \"lr\": trial.suggest_float('lr', 1e-3, 1e-1),\n",
    "                \"lr_decay\": trial.suggest_float('lr_decay', 1e-2, 9.99e-1),\n",
    "                \"lr_decay_steps\": trial.suggest_int(\"lr_decay_steps\", 100, 1000, step=100),\n",
    "                \"optimizer\": 'adam',\n",
    "                \"max_steps\": trial.suggest_int(\"max_steps\", 100, 10000, log=True),\n",
    "                \"batch_size\": trial.suggest_int(\"batch_size\", 100, 10000, log=True),\n",
    "                \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True),\n",
    "                \"pt_model_kwargs\": {\n",
    "                    'input_dim': 85,\n",
    "                    'layers': layers,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    evals_result = dict()\n",
    "    model = init_instance_by_config(task[\"model\"])\n",
    "    model.fit(dataset, evals_result=evals_result)\n",
    "    pred = model.predict(dataset, segment='test')\n",
    "    pred_label = pd.concat([pred, label], axis=1)\n",
    "    ic = pred_label.groupby(level=0).corr()\n",
    "    ic = ic.loc[(slice(None), ic.columns[0]), ic.columns[-1]].mean()\n",
    "    return ic\n",
    "\n",
    "name = 'mlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name = name, \n",
    "    direction='maximize', \n",
    "    storage=f'sqlite:///{name}.db',\n",
    "    load_if_exists=True,\n",
    ")\n",
    "study = optuna.Study(study_name=name, storage=f'sqlite:///{name}.db')\n",
    "study.optimize(objective_mlp, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analyze\n",
    "\n",
    "After the parameters tuning, our trial result and parameters are stored in the db data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.Study(study_name=name, storage=f'sqlite:///{name}.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as optviz\n",
    "\n",
    "# optviz.plot_optimization_history(study)\n",
    "# optviz.plot_parallel_coordinate(study)\n",
    "# optviz.plot_contour(study, params=['max_depth', 'num_leaves'])\n",
    "optviz.plot_slice(study, params=['batch_size'])\n",
    "# optviz.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add epoch end evaluation, use layering return as a indicator\n",
    "# use adamw optimizer\n",
    "# reduce data loss, use k fold, merge the valid and test dataset into one\n",
    "# maybe use open price can perform better\n",
    "# CNN without pooling\n",
    "# TabNet has better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('qlib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b061070c2a4f0a17beb221bbb485cf11738fa39836b1ba79e3ea50588350e39e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
