{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Factor Digging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "from dask.multiprocessing import get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kline_min_dir = '../data/kline-min/'\n",
    "kline_day_path = '../data/kline-daily/market-daily.parquet'\n",
    "feature_dir = '../data/features/'\n",
    "klines = pd.read_parquet(kline_day_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Simple Mometum\n",
    "\n",
    " equals to the return in a period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum(close_price: pd.Series, period: int) -> pd.DataFrame:\n",
    "    factor = close_price.groupby(level=1).pct_change(periods=period).dropna()\n",
    "    return factor.unstack()\n",
    "\n",
    "# momentum_5d = momentum(klines.adjclose, 5)\n",
    "# momentum_5d.to_parquet(feature_dir + 'momentum_5d.parquet')\n",
    "# momentum_10d = momentum(klines.adjclose, 10)\n",
    "# momentum_10d.to_parquet(feature_dir + \"momentum_10d.parquet\")\n",
    "# momentum_20d = momentum(klines.adjclose, 20)\n",
    "# momentum_20d.to_parquet(feature_dir + \"momentum_20d.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Turover Weighted Momentum\n",
    "\n",
    " Daily return weighted by turnover of each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover = pd.read_parquet('../data/derivative-indicators/turnover.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wgt_momentum(turnover: pd.Series, close_price: pd.Series, period: int):\n",
    "    ret = close_price.groupby(level=1).pct_change(1)\n",
    "    factor = (turnover * ret).groupby(level=1).rolling(period).sum()\n",
    "    return factor.droplevel(0).unstack().dropna(how='all')\n",
    "\n",
    "wgt_momentum_5d = wgt_momentum(turnover.today, klines.adjclose, 5)\n",
    "wgt_momentum_5d.to_parquet(feature_dir + 'wgt_momentum_5d.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exponential Turnover Weighted Momentum\n",
    "\n",
    " Equals to the $ \\Sigma_{t=T-period}^{T} turnover_t * {dailyreturn}_t * exp^{- \\frac{T - t} {N * 4}}$, where t represents the cross section date, T is the calculating date and N is total time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover = pd.read_parquet('../input/derivative-indicators/turnover.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_wgt_momentum(turnover: pd.Series, close_price: pd.Series, period: int):\n",
    "    def _calc(x):\n",
    "        return (x * np.exp(-np.arange(period) / (4 * period))[::-1]).sum()\n",
    "        \n",
    "    ret = close_price.groupby(level=1).pct_change(1)\n",
    "    factor = (turnover * ret).groupby(level=1).rolling(period, min_periods=period).apply(_calc)\n",
    "    return factor.droplevel(0).dropna().unstack()\n",
    "\n",
    "# exp_wgt_momentum_5d = exp_wgt_momentum(turnover.today, klines.adjclose, 5)\n",
    "# exp_wgt_momentum_5d.to_parquet(feature_dir + 'exp_wgt_momentum_5d.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Turnover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Simple Turnover\n",
    "\n",
    " Equals to the turnover in a period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turnover = pd.read_parquet('../input/derivative-indicators/turnover.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turnover_(turnover: pd.Series, period: int):\n",
    "    factor = turnover.groupby(level=1).rolling(period).mean()\n",
    "    return factor.droplevel(0).dropna().unstack()\n",
    "\n",
    "# turnover_5d = turnover_(turnover.today, period=5)\n",
    "# turnover_5d.to_parquet(feature_dir + 'turnover_5d.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Turnover Standard Deviation\n",
    "\n",
    " The standard deviation of turnover in a period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_turnover(turnover: pd.Series, period: int):\n",
    "    factor = turnover.groupby(level=1).rolling(period).std()\n",
    "    return factor.droplevel(0).dropna().unstack()\n",
    "\n",
    "# std_turnover_5d = std_turnover(turnover.today, period=5)\n",
    "# std_turnover_5d.to_parquet(feature_dir + 'std_turnover_5d.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Turnover Trend\n",
    "\n",
    " The moving average of turnover in a short period over a longer period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_turnover(turnover: pd.Series, short: int, long: int):\n",
    "    short_turn = turnover.groupby(level=1).rolling(short).mean()\n",
    "    long_turn = turnover.groupby(level=1).rolling(long).mean()\n",
    "    factor = short_turn / long_turn - 1\n",
    "    return factor.droplevel(0).dropna().unstack()\n",
    "\n",
    "# trend_turnover_5d_10d = trend_turnover(turnover.today, 5, 10)\n",
    "# trend_turnover_5d_10d.to_parquet(feature_dir + 'trend_turnover_5d_10d.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Turnover Standard Deviation Trend\n",
    "\n",
    " The moving average of turnover deviation in a short period over a longer period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_deviation_turnover(turnover: pd.Series, short: int, long: int):\n",
    "    short_turn = turnover.groupby(level=1).rolling(short).std()\n",
    "    long_turn = turnover.groupby(level=1).rolling(long).std()\n",
    "    factor = short_turn / long_turn - 1\n",
    "    return factor.droplevel(0).dropna().unstack()\n",
    "\n",
    "# trend_deviation_turnover_5d_10d = trend_deviation_turnover(turnover.today, 5, 10)\n",
    "# trend_deviation_turnover_5d_10d.to_parquet(feature_dir + 'trend_deviation_turnover_5d_10d.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Return Deviation\n",
    "\n",
    " The deviation of daily return in a period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_deviation(close_price: pd.Series, period: int):\n",
    "    ret = close_price.groupby(level=1).pct_change(1)\n",
    "    factor = ret.groupby(level=1).rolling(period).std()\n",
    "    return factor.droplevel(0).dropna().unstack()\n",
    "\n",
    "ret_deviation_5d = ret_deviation(klines.adjclose, 5)\n",
    "ret_deviation_5d.to_parquet(feature_dir + 'ret_deviation_5d.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capital Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgt = pd.read_parquet('../data/derivative-indicators/hsg_holding.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lu Gu Tong momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgt_momentum(holding: pd.DataFrame, period: int = 5):\n",
    "    factor = holding.pct_change(period)\n",
    "    return factor\n",
    "\n",
    "lgt_momentum_5d = lgt_momentum(lgt.shares_holding.unstack(level=0), period=5)\n",
    "lgt_momentum_5d.astype('float32').to_parquet(feature_dir + 'lgt_momentum_5d.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lu Gu Tong holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgt_change(holding: pd.Series, period: int = 5):\n",
    "    factor = holding.rolling(period).apply(\n",
    "        lambda x: (x.iloc[-1] - x.min()) / (x.max() - x.min()) if x.max() != x.min() else 0)\n",
    "    return factor.dropna(how='all')\n",
    "\n",
    "lgt_change_20d = lgt_change(lgt.shares_holding.unstack(level=0), 20)\n",
    "lgt_change_20d.astype('float32').to_parquet(feature_dir + 'lgt_change_20d.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Minute Data Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minute Price correlation with Volume\n",
    "\n",
    "Equals to the correlation coeficient of minute price and volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(filter(lambda x: x.endswith('.parquet'), os.listdir(kline_min_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pvcorr(data_path: str):\n",
    "    data = pd.read_parquet(data_path, columns=['volume', 'adjclose'])\n",
    "    factor = data.groupby(level=0).apply(\n",
    "        lambda x: x.droplevel(0).groupby(\n",
    "            lambda y: y.date).apply(\n",
    "                lambda z: z['adjclose'].corr(z['volume'])))\n",
    "    return factor\n",
    "\n",
    "dsk = dict(zip([date[:6] for date in dates], [(pvcorr, kline_min_dir + date) for date in dates]))\n",
    "chunck_size = 20\n",
    "all_keys = list(dsk.keys())\n",
    "results = get(dsk, all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nresults = []\n",
    "for result in results:\n",
    "    if len(result.shape) == 1:\n",
    "        result = result.unstack(level=0)\n",
    "    elif len(result.shape) == 2:\n",
    "        result = result.T\n",
    "    nresults.append(result)\n",
    "nresults = pd.concat(nresults, axis=0)\n",
    "nresults = nresults.dropna(axis=1, how='all')\n",
    "nresults.index = pd.to_datetime(nresults.index)\n",
    "nresults = nresults.sort_index()\n",
    "nresults.astype('float32').to_parquet(feature_dir + 'pvcorr_1d.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Factor Test\n",
    " First calculate the forward return:\n",
    "\n",
    " Suppose we are on day *T*, and after the end of the trading hours, we\n",
    " can calculate factor values, and use the value to select stock.\n",
    " So the fastest time we can buy the stock is the open price of the\n",
    " next day.\n",
    "\n",
    " Suppose we hold the stock for *period* days, so, on day *T + 1 + period*\n",
    " we sell the stock at the close.\n",
    " In conclusion, the way we calculate the forward return is:\n",
    "\n",
    " $ close_{T + 1 + period} / open_{T + 1} - 1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"2010-01-04\"\n",
    "end = \"2022-01-04\"\n",
    "factor_name = 'ret_deviation_5d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 1\n",
    "forward = klines.adjclose.groupby(level=1).shift(-1 - period) / klines.adjopen.groupby(level=1).shift(-1) - 1\n",
    "forward = forward.dropna().unstack()\n",
    "forward = forward.loc[start:end]\n",
    "forward.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Factor Preprocess\n",
    " When we are processing data, we applied deextreme and standarization.\n",
    " We didn't apply empty value process because we will stack and dropna automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = pd.read_parquet(feature_dir + factor_name + '.parquet')\n",
    "factor = factor.loc[start:end]\n",
    "\n",
    "factor_med = factor.median(axis=1)\n",
    "mad = factor.subtract(factor_med, axis=0).abs().median(axis=1)\n",
    "\n",
    "factor = factor.clip(factor_med - 9 * mad, factor_med + 9 * mad, axis=0)\n",
    "factor = factor.subtract(factor.mean(axis=1), axis=0).divide(factor.std(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Cross Section Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "common_index = factor.index.intersection(forward.index)\n",
    "cross_section_period = common_index[23]\n",
    "\n",
    "concated = pd.concat([factor.loc[cross_section_period], forward.loc[cross_section_period]], axis=1)\n",
    "concated.columns = ['factor', 'forward']\n",
    "_ = scatter_matrix(concated, figsize=(12, 12), hist_kwds={'bins': 100})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### IC Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic = factor.corrwith(forward, axis=1, method='spearman').dropna()\n",
    "ax = ic.plot.bar(figsize=(28, 9))\n",
    "ax.set_xticks(ax.get_xticks()[::100])\n",
    "ax.plot(ic.rolling(10).mean().values, color='darkred')\n",
    "_ = ax.set_xticklabels(ic.index.strftime('%b\\n%Y')[::100], rotation=45)\n",
    "ax.hlines(y=-0.03, xmax=ax.get_xlim()[-1], xmin=ax.get_xlim()[0], color='grey', linestyle=\":\")\n",
    "ax.hlines(y=0.03, xmax=ax.get_xlim()[-1], xmin=ax.get_xlim()[0], color='grey', linestyle=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.groupby(lambda x: x.year).mean().plot.bar(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.mean(), ic.std(), ic.mean() / ic.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_1samp(ic, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Layering Test\n",
    " In layering test, we quantilize the factor into multiple group by\n",
    " its value, and hold the group as a portfolio to see the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantilize the factor data\n",
    "quantiles = factor.stack().groupby(level=0).apply(pd.qcut, q=5, labels=False) + 1\n",
    "quantiles = quantiles.loc[quantiles.unstack().index.intersection(forward.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Construct a portfolio by quantiles, `weights` is the normalized weight.\n",
    " And then calculate profit for each quantile, in the group of each quantile,\n",
    " we calculate return day by day, however, due to the influence of\n",
    " relocation period, we should split our money into period quantiles too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.Series(np.ones_like(quantiles), index=quantiles.index)\n",
    "weights = weights.groupby([quantiles, pd.Grouper(level=0)]).apply(lambda x: x / x.sum() * 1 / (period + 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit = weights.groupby([quantiles, pd.Grouper(level=0)]).apply(\n",
    "    lambda x: (x * forward.loc[x.index.get_level_values(0)[0]]).sum()\n",
    ")\n",
    "profit = pd.concat([profit.unstack(level=0), forward.mean(axis=1) * 1 / (1 + period)], axis=1)\n",
    "profit.columns = profit.columns[:-1].to_list() + ['simple_buy_hold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean factor returns in each quantile\n",
    "ax = profit.groupby(lambda x: x.year).apply(lambda x: (x + 1).cumprod().iloc[-1]).plot.bar(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Trade by a high frequency, the trading\n",
    " commision cannot be neglected, here is how to\n",
    " calculate the turnover rate and add commisions\n",
    " on both sides (buy and sell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_weights = weights.unstack().stack(dropna=False).fillna(0)\n",
    "turnovers = []\n",
    "for i in range(period + 1):\n",
    "    subport_date = panel_weights.index.levels[0][i::(period + 1)]\n",
    "    turnover = panel_weights.loc[subport_date].groupby(quantiles).apply(\n",
    "        lambda x: (x - x.groupby(level=1).shift(1).fillna(0))\\\n",
    "            .groupby(level=0).apply(lambda y: y.abs().sum()\n",
    "        )\n",
    "    )\n",
    "    turnovers.append(turnover)\n",
    "turnover = pd.concat(turnovers, axis=0).sort_index()\n",
    "turnover = turnover.unstack(level=0)\n",
    "turnover = pd.concat([turnover, pd.Series(np.zeros(turnover.shape[0]), \n",
    "    index=turnover.index, name='simple_buy_hold')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we subtract the commision from the original profit,\n",
    "and we set the commision rate to 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commission_ratio = 0.003\n",
    "profit -= commission_ratio * turnover\n",
    "cumprofit = (profit.shift(period + 1).fillna(0) + 1).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumprofit.plot(figsize=(12, 8), cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(turnover * 100).plot(figsize=(12, 8), ylabel='Turnover%', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('qlib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b061070c2a4f0a17beb221bbb485cf11738fa39836b1ba79e3ea50588350e39e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
